---
#File-Name:       Corpus_Fallos.R
#Date:            12.01.2015
#Author:          Federico Carlés
#Email:           fedecarles@gmail.com                                      
#Data:            www.scba.gov.ar
#Packages Used:   XML, stringr
#R version        3.1.2 (2014-10-31)
#System           Linux Mint 17 (Qiana)
#Encoding         UTF-8
---

##Corpus de Fallos de la Corte Suprema de Buenos Aires



El código a continuación utiliza el listado de urls de los fallos completos de la SCBA para extraer el texto de los mismos y archivar cada uno en un archivo txt separado. Estos archivos constituirán el *Corpus* sobre el cual se minarán datos. 

* El primer paso es cargar las librerias y datos a usar.
```{r}
setwd("~/Documents/Proyecto SCBA/R")
library(XML)
require(stringr)

url_file <- read.csv("urls_fallos_list.csv")
urls <- as.character(url_file[1:5,])
```

* El segundo paso es extraer información de las urls para construir lo que será el nombre de los archivos. Se utilizan expresiones regulares para extraer los caracteres que servirán de identificadores (el año y el número del fallo).
```{r}
file_name <- str_extract_all(urls, pattern = "(([0-9]{4}(.*))\\.doc)" )
file_name <- gsub(pattern="\\/", replace = "-", x = file_name)
file_name <- gsub(pattern="\\.doc", replace = "", x = file_name)
```

* El tercer paso es extraer el texto de cada fallo mediante un loop que corre sobre toda la lista de urls. Se utiliza el identificador XPATH para indicar que parte del contenido del código HTML queremos extraer. Los reultados se guardan en una lista.
```{r}
text <- list()
for (i in urls)
  text[i] <- xpathSApply(htmlParse(i), "//*[contains(@class, 'mini')]", xmlValue)
```

* El cuarto paso es guardar cada texto extraído en un archivo txt separado.
```{r}
for (x in 1:5)
  write.table(text[[x]], file=paste(file_name[x],"-",x, "txt", sep="."))
```